{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Learning of Key Phrases and Topics in Document Collections\n",
    "\n",
    "## Part 2: Phrase Learning\n",
    "\n",
    "<font color='#053582'>\n",
    "<br>\n",
    "In this section, we will apply phrase learning, or N-gram learning, opening the door for topics being distributions not over all words but over these learned phrases as well - e.g the latent topic (security, gun, mall, guard) could be supplemented nicely with a phrase such as metal_detector, and the topic (security, cyber, virus, anomaly, detection, hacker, backdoor) might be better described as (cyber_security, virus, anomaly_detection, hacker, backdoor).\n",
    "<br>\n",
    "We will discover the best phrases to use via calculation of the Weighted Average Pointwise Mutual Information of possible phrases.\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "import re\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from math import log\n",
    "from sys import getsizeof\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import platform\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFrame = pandas.read_csv(os.path.join(\"./Data\", 'CongressionalDocsCleaned.tsv'), \n",
    "                            sep='\\t', \n",
    "                            encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in cleaned text: 5536249\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Provides that effective from January 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>2</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>3</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>4</td>\n",
       "      <td>of the Ninety-second Congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>1</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>2</td>\n",
       "      <td>acting through the United States delegation to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>3</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>4</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>5</td>\n",
       "      <td>as may be appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>6</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>7</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>8</td>\n",
       "      <td>chemical munitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>9</td>\n",
       "      <td>military material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>10</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>11</td>\n",
       "      <td>contiguous zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>12</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>13</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>1</td>\n",
       "      <td>States that it shall be the duty of the commit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>2</td>\n",
       "      <td>and to report to the Congress its findings in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>3</td>\n",
       "      <td>Authorizes the committee to make such expendit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>4</td>\n",
       "      <td>hold such hearings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>5</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID  DocLine                                        CleanedText\n",
       "0   hconres1-93        0             Provides that effective from January 3\n",
       "1   hconres1-93        1                                               1973\n",
       "2   hconres1-93        2  the joint committee created to make the necess...\n",
       "3   hconres1-93        3  is hereby continued and for such purpose shall...\n",
       "4   hconres1-93        4                      of the Ninety-second Congress\n",
       "5   hconres2-93        0  Makes it the sense of the Congress that the po...\n",
       "6   hconres2-93        1  Makes it the sense of the Congress that the Pr...\n",
       "7   hconres2-93        2  acting through the United States delegation to...\n",
       "8   hconres2-93        3  should take such steps as may be necessary to ...\n",
       "9   hconres2-93        4  or amendments to existing international agreem...\n",
       "10  hconres2-93        5                              as may be appropriate\n",
       "11  hconres2-93        6  providing for coordinated international activi...\n",
       "12  hconres2-93        7                                          chemicals\n",
       "13  hconres2-93        8                                 chemical munitions\n",
       "14  hconres2-93        9                                  military material\n",
       "15  hconres2-93       10           and any pollutants in territorial waters\n",
       "16  hconres2-93       11                                   contiguous zones\n",
       "17  hconres2-93       12        the deep seabed or any international waters\n",
       "18  hconres2-93       13  and otherwise to prevent the pollution of the ...\n",
       "19  hconres3-93        0  Establishes a Joint Congressional Committee on...\n",
       "20  hconres3-93        1  States that it shall be the duty of the commit...\n",
       "21  hconres3-93        2  and to report to the Congress its findings in ...\n",
       "22  hconres3-93        3  Authorizes the committee to make such expendit...\n",
       "23  hconres3-93        4                                 hold such hearings\n",
       "24  hconres3-93        5            and employ such assistants as necessary"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Total lines in cleaned text: %d\\n\" % len(textFrame))\n",
    "\n",
    "# Show the first 25 rows of the data in the frame\n",
    "textFrame[0:25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lowercase Version of the Text Data\n",
    "\n",
    "Before learning phrases we lowercase the entire text corpus to ensure all casing variants for each word are collapsed into a single uniform variant used during the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Provides that effective from January 3</td>\n",
       "      <td>provides that effective from january 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>2</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "      <td>the joint committee created to make the necess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>3</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "      <td>is hereby continued and for such purpose shall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>4</td>\n",
       "      <td>of the Ninety-second Congress</td>\n",
       "      <td>of the ninety-second congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Makes it the sense of the Congress that the po...</td>\n",
       "      <td>makes it the sense of the congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>1</td>\n",
       "      <td>Makes it the sense of the Congress that the Pr...</td>\n",
       "      <td>makes it the sense of the congress that the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>2</td>\n",
       "      <td>acting through the United States delegation to...</td>\n",
       "      <td>acting through the united states delegation to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>3</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "      <td>should take such steps as may be necessary to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>4</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "      <td>or amendments to existing international agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>5</td>\n",
       "      <td>as may be appropriate</td>\n",
       "      <td>as may be appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>6</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>7</td>\n",
       "      <td>chemicals</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>8</td>\n",
       "      <td>chemical munitions</td>\n",
       "      <td>chemical munitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>9</td>\n",
       "      <td>military material</td>\n",
       "      <td>military material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>10</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>11</td>\n",
       "      <td>contiguous zones</td>\n",
       "      <td>contiguous zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>12</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>13</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "      <td>establishes a joint congressional committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>1</td>\n",
       "      <td>States that it shall be the duty of the commit...</td>\n",
       "      <td>states that it shall be the duty of the commit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>2</td>\n",
       "      <td>and to report to the Congress its findings in ...</td>\n",
       "      <td>and to report to the congress its findings in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>3</td>\n",
       "      <td>Authorizes the committee to make such expendit...</td>\n",
       "      <td>authorizes the committee to make such expendit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>4</td>\n",
       "      <td>hold such hearings</td>\n",
       "      <td>hold such hearings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>5</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "      <td>and employ such assistants as necessary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID  DocLine                                        CleanedText  \\\n",
       "0   hconres1-93        0             Provides that effective from January 3   \n",
       "1   hconres1-93        1                                               1973   \n",
       "2   hconres1-93        2  the joint committee created to make the necess...   \n",
       "3   hconres1-93        3  is hereby continued and for such purpose shall...   \n",
       "4   hconres1-93        4                      of the Ninety-second Congress   \n",
       "5   hconres2-93        0  Makes it the sense of the Congress that the po...   \n",
       "6   hconres2-93        1  Makes it the sense of the Congress that the Pr...   \n",
       "7   hconres2-93        2  acting through the United States delegation to...   \n",
       "8   hconres2-93        3  should take such steps as may be necessary to ...   \n",
       "9   hconres2-93        4  or amendments to existing international agreem...   \n",
       "10  hconres2-93        5                              as may be appropriate   \n",
       "11  hconres2-93        6  providing for coordinated international activi...   \n",
       "12  hconres2-93        7                                          chemicals   \n",
       "13  hconres2-93        8                                 chemical munitions   \n",
       "14  hconres2-93        9                                  military material   \n",
       "15  hconres2-93       10           and any pollutants in territorial waters   \n",
       "16  hconres2-93       11                                   contiguous zones   \n",
       "17  hconres2-93       12        the deep seabed or any international waters   \n",
       "18  hconres2-93       13  and otherwise to prevent the pollution of the ...   \n",
       "19  hconres3-93        0  Establishes a Joint Congressional Committee on...   \n",
       "20  hconres3-93        1  States that it shall be the duty of the commit...   \n",
       "21  hconres3-93        2  and to report to the Congress its findings in ...   \n",
       "22  hconres3-93        3  Authorizes the committee to make such expendit...   \n",
       "23  hconres3-93        4                                 hold such hearings   \n",
       "24  hconres3-93        5            and employ such assistants as necessary   \n",
       "\n",
       "                                        LowercaseText  \n",
       "0              provides that effective from january 3  \n",
       "1                                                1973  \n",
       "2   the joint committee created to make the necess...  \n",
       "3   is hereby continued and for such purpose shall...  \n",
       "4                       of the ninety-second congress  \n",
       "5   makes it the sense of the congress that the po...  \n",
       "6   makes it the sense of the congress that the pr...  \n",
       "7   acting through the united states delegation to...  \n",
       "8   should take such steps as may be necessary to ...  \n",
       "9   or amendments to existing international agreem...  \n",
       "10                              as may be appropriate  \n",
       "11  providing for coordinated international activi...  \n",
       "12                                          chemicals  \n",
       "13                                 chemical munitions  \n",
       "14                                  military material  \n",
       "15           and any pollutants in territorial waters  \n",
       "16                                   contiguous zones  \n",
       "17        the deep seabed or any international waters  \n",
       "18  and otherwise to prevent the pollution of the ...  \n",
       "19  establishes a joint congressional committee on...  \n",
       "20  states that it shall be the duty of the commit...  \n",
       "21  and to report to the congress its findings in ...  \n",
       "22  authorizes the committee to make such expendit...  \n",
       "23                                 hold such hearings  \n",
       "24            and employ such assistants as necessary  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a lowercased version of the data and add it into the data frame\n",
    "lowercaseText = []\n",
    "for textLine in textFrame['CleanedText']:\n",
    "    lowercaseText.append(str(textLine).lower())\n",
    "textFrame['LowercaseText'] = lowercaseText;           \n",
    "            \n",
    "textFrame[0:25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Supplemental Word Lists\n",
    "\n",
    "Words in the black list are completely ignored by the process and cannot be used in the creation of phrases. Words in the function word list can only be used in between content words in the creation of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function for loading lists into dictionary hash tables\n",
    "def LoadListAsHash(filename):\n",
    "    listHash = {}\n",
    "    fp = open(filename, encoding='utf-8')\n",
    "\n",
    "    # Read in lines one by one stripping away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in fp.readlines():\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "\n",
    "    fp.close()\n",
    "    return listHash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the black list of words to ignore \n",
    "blacklistHash = LoadListAsHash(os.path.join(\"./Data\", 'black_list.txt'))\n",
    "\n",
    "# Load the list of non-content bearing function words\n",
    "functionwordHash = LoadListAsHash(os.path.join(\"./Data\", 'function_words.txt'))\n",
    "\n",
    "# Add more terms to the function word list\n",
    "functionwordHash[\"foo\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute N-gram Statistics for Phrase Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "Here we will compute the counts for all valid 1-4 grams (no function/black listed words, etc.)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function that used to define how to compute N-gram stats\n",
    "# This function will be executed once-per-cpu core,in parallel, using a process pool executor\n",
    "def ComputeNgramStatsJob(textList, functionwordHash, blacklistHash, reValidWord, jobId, verbose=False):\n",
    "    if verbose:\n",
    "        startTS = datetime.now()\n",
    "        print(\"[%s] Starting batch execution %d\" % (str(startTS), jobId+1))\n",
    "    \n",
    "    # Create an array to store the total count of all ngrams up to 4-grams\n",
    "    # Array element 0 is unused, element 1 is unigrams, element 2 is bigrams, etc.\n",
    "    ngramCounts = [0]*5;\n",
    "       \n",
    "    # Create a list of structures to tabulate ngram count statistics\n",
    "    # Array element 0 is the array of total ngram counts,\n",
    "    # Array element 1 is a hash table of individual unigram counts\n",
    "    # Array element 2 is a hash table of individual bigram counts\n",
    "    # Array element 3 is a hash table of individual trigram counts\n",
    "    # Array element 4 is a hash table of individual 4-gram counts\n",
    "    ngramStats = [ngramCounts, {}, {}, {}, {}]\n",
    "    \n",
    "    numLines = len(textList)\n",
    "    if verbose:\n",
    "        print(\"# Batch %d, received %d lines data\" % (jobId+1, numLines))\n",
    "    \n",
    "    for i in range(0, numLines):\n",
    "        # Split the text line into an array of words\n",
    "        wordArray = textList[i].strip().split()\n",
    "        numWords = len(wordArray)\n",
    "        \n",
    "        # Create an array marking each word as valid or invalid\n",
    "        validArray = [reValidWord.match(word) != None for word in wordArray]\n",
    "        \n",
    "        # Tabulate total raw ngrams for this line into counts for each ngram bin\n",
    "        # The total ngrams counts include the counts of all ngrams including those\n",
    "        # that we won't consider as parts of phrases\n",
    "        for j in range(1, 5):\n",
    "            if j <= numWords:\n",
    "                ngramCounts[j] += numWords - j + 1\n",
    "        \n",
    "        # Collect counts for viable phrase ngrams and left context sub-phrases\n",
    "        for j in range(0, numWords):\n",
    "            word = wordArray[j]\n",
    "\n",
    "            # Only bother counting the ngrams that start with a valid content word\n",
    "            # i.e., valid words not in the function word list or the black list\n",
    "            if ( ( word not in functionwordHash ) and ( word not in blacklistHash ) and validArray[j] ):\n",
    "\n",
    "                # Initialize ngram string with first content word and add it to unigram counts\n",
    "                ngramSeq = word \n",
    "                if ngramSeq in ngramStats[1]:\n",
    "                    ngramStats[1][ngramSeq] += 1\n",
    "                else:\n",
    "                    ngramStats[1][ngramSeq] = 1\n",
    "\n",
    "                # Count valid ngrams from bigrams up to 4-grams\n",
    "                stop = 0\n",
    "                k = 1\n",
    "                while (k<4) and (j+k<numWords) and not stop:\n",
    "                    n = k + 1\n",
    "                    nextNgramWord = wordArray[j+k]\n",
    "                    # Only count ngrams with valid words not in the blacklist\n",
    "                    if ( validArray[j+k] and nextNgramWord not in blacklistHash ):\n",
    "                        ngramSeq += \" \" + nextNgramWord\n",
    "                        if ngramSeq in ngramStats[n]:\n",
    "                            ngramStats[n][ngramSeq] += 1\n",
    "                        else:\n",
    "                            ngramStats[n][ngramSeq] = 1 \n",
    "                        k += 1\n",
    "                        if nextNgramWord not in functionwordHash:\n",
    "                            # Stop counting new ngrams after second content word in \n",
    "                            # ngram is reached and ngram is a viable full phrase\n",
    "                            stop = 1\n",
    "                    else:\n",
    "                        stop = 1\n",
    "    \n",
    "    if verbose:\n",
    "        endTS = datetime.now()\n",
    "        delta_t = (endTS - startTS).total_seconds()\n",
    "        print(\"[%s] Batch %d finished, time elapsed: %f seconds\" % (str(endTS), jobId+1, delta_t))\n",
    "    \n",
    "    return ngramStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is Step 1 for each iteration of phrase learning\n",
    "# We count the number of occurrences of all 2-gram, 3-ngram, and 4-gram\n",
    "# word sequences \n",
    "def ComputeNgramStats(textData, functionwordHash, blacklistHash, numWorkers, verbose=False):\n",
    "          \n",
    "    # Create a regular expression for assessing validity of words\n",
    "    # for phrase modeling. The expression says words in phrases\n",
    "    # must either:\n",
    "    # (1) contain an alphabetic character, or \n",
    "    # (2) be the single charcater '&', or\n",
    "    # (3) be a one or two digit number\n",
    "    reWordIsValid = re.compile('[A-Za-z]|^&$|^\\d\\d?$');\n",
    "    \n",
    "    # Go through the text data line by line collecting count statistics\n",
    "    # for all valid n-grams that could appear in a potential phrase\n",
    "    numLines = len(textData)\n",
    "    \n",
    "    # Get the number of CPU to run the tasks\n",
    "    if numWorkers > cpu_count() or numWorkers <= 0:\n",
    "        worker = cpu_count()\n",
    "    else:\n",
    "        worker = numWorkers\n",
    "    if verbose:\n",
    "        print(\"Worker size = %d\" % worker)\n",
    "    \n",
    "    # Get the batch size for each execution job\n",
    "    # The very last job executor may received more lines of data\n",
    "    batch_size = int(numLines/worker)\n",
    "    batchIndexes = range(0, numLines, batch_size)\n",
    "    \n",
    "    batch_returns = []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=worker) as executor:\n",
    "        jobs = set()\n",
    "        \n",
    "        # Map the task into multiple batch executions\n",
    "        if platform.system() == \"Linux\" or platform.system() == \"Darwin\":\n",
    "            for idx in range(worker):\n",
    "                # The very last job executor\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(executor.submit(ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]: ], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx, \n",
    "                                                 verbose))\n",
    "                else:\n",
    "                    jobs.add(executor.submit(ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx,\n",
    "                                                 verbose))\n",
    "        else:\n",
    "            # For Windows system, it is different to handle ProcessPoolExecutor\n",
    "            from misc import winprocess\n",
    "            \n",
    "            for idx in range(worker):\n",
    "                # The very last job executor\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                                 ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]: ], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx, \n",
    "                                                 verbose))\n",
    "                else:\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                                 ComputeNgramStatsJob, \n",
    "                                                 textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                                 functionwordHash, \n",
    "                                                 blacklistHash,\n",
    "                                                 reWordIsValid,\n",
    "                                                 idx,\n",
    "                                                 verbose))\n",
    "        \n",
    "        # Get results from batch executions\n",
    "        for job in concurrent.futures.as_completed(jobs):\n",
    "            try:\n",
    "                ret = job.result()\n",
    "            except Exception as e:\n",
    "                print(\"Generated an exception while trying to get result from a batch: %s\" % e)\n",
    "            else:\n",
    "                batch_returns.append(ret)\n",
    "\n",
    "    # Reduce the results from batch executions\n",
    "    # Reuse the first return\n",
    "    ngramStats = batch_returns[0]\n",
    "    \n",
    "    for batch_id in range(1, len(batch_returns)):\n",
    "        result = batch_returns[batch_id]\n",
    "        \n",
    "        # Update the ngram counts\n",
    "        ngramStats[0] = [x + y for x, y in zip(ngramStats[0], result[0])]\n",
    "        \n",
    "        # Update the hash table of ngram counts\n",
    "        for n_gram in range(1, 5):\n",
    "            for item in result[n_gram]:\n",
    "                if item in ngramStats[n_gram]:\n",
    "                    ngramStats[n_gram][item] += result[n_gram][item]\n",
    "                else:\n",
    "                    ngramStats[n_gram][item] = result[n_gram][item]\n",
    "    \n",
    "    return ngramStats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Potential Phrases by the Weighted Pointwise Mutual Information of their Constituent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "What this  means is, we break up all n-grams that we have collected to far, and score them based on how informative we expect them to be as phrases. This is done by computing the weighted average pointwise mutual information (WAPMI) of the phrase, which is a common feature score in feature selection for text categorization, and is an improvement over the usual https://en.wikipedia.org/wiki/Mutual_information.\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RankNgrams(ngramStats,functionwordHash,minCount):\n",
    "    # Create a hash table to store weighted pointwise mutual \n",
    "    # information scores for each viable phrase\n",
    "    ngramWPMIHash = {}\n",
    "        \n",
    "    # Go through each of the ngram tables and compute the phrase scores\n",
    "    # for the viable phrases\n",
    "    for n in range(2,5):\n",
    "        i = n-1\n",
    "        for ngram in ngramStats[n].keys():\n",
    "            ngramCount = ngramStats[n][ngram]\n",
    "            if ngramCount >= minCount:\n",
    "                wordArray = ngram.split()\n",
    "                # If the final word in the ngram is not a function word then\n",
    "                # the ngram is a valid phrase candidate we want to score\n",
    "                if wordArray[i] not in functionwordHash: \n",
    "                    leftNgram = ' '.join(wordArray[:-1])\n",
    "                    rightWord = wordArray[i]\n",
    "                    \n",
    "                    # Compute the weighted pointwise mutual information (WPMI) for the phrase\n",
    "                    probNgram = float(ngramStats[n][ngram])/float(ngramStats[0][n])\n",
    "                    probLeftNgram = float(ngramStats[n-1][leftNgram])/float(ngramStats[0][n-1])\n",
    "                    probRightWord = float(ngramStats[1][rightWord])/float(ngramStats[0][1])\n",
    "                    WPMI = probNgram * math.log(probNgram/(probLeftNgram*probRightWord));\n",
    "\n",
    "                    # Add the phrase into the list of scored phrases only if WMPI is positive\n",
    "                    if WPMI > 0:\n",
    "                        ngramWPMIHash[ngram] = WPMI  \n",
    "    \n",
    "    # Create a sorted list of the phrase candidates\n",
    "    rankedNgrams = sorted(ngramWPMIHash, key=ngramWPMIHash.__getitem__, reverse=True)\n",
    "\n",
    "    # Force a memory clean-up\n",
    "    ngramWPMIHash = None\n",
    "    gc.collect()\n",
    "\n",
    "    return rankedNgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Phrase Rewrites to Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "Here we will replace instances of n-grams with a single-word representation, using an underscore, i.e. cyber security becomes cyber_security and is henceforth thought of as a single word. This is crucial for Topic Modeling, because gensim's LDA does not support n-grams in their topics. They do however, offer a package for phrase detection - models.phrases.Phrases which we will not use.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phraseRewriteJob(ngramRegex, text, ngramRewriteHash, jobId, verbose=True):\n",
    "    if verbose:\n",
    "        startTS = datetime.now()\n",
    "        print(\"[%s] Starting batch execution %d\" % (str(startTS), jobId+1))\n",
    "    \n",
    "    retList = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        # The regex substitution looks up the output string rewrite\n",
    "        # in the hash table for each matched input phrase regex\n",
    "        retList.append(ngramRegex.sub(lambda mo: ngramRewriteHash[mo.string[mo.start():mo.end()]], text[i]))\n",
    "    \n",
    "    if verbose:\n",
    "        endTS = datetime.now()\n",
    "        delta_t = (endTS - startTS).total_seconds()\n",
    "        print(\"[%s] Batch %d finished, batch size: %d, time elapsed: %f seconds\" % (str(endTS), jobId+1, i, delta_t))\n",
    "    \n",
    "    return retList, jobId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewrites(rankedNgrams, textData, learnedPhrases, maxPhrasesToAdd, \n",
    "                        maxPhraseLength, verbose, numWorkers=cpu_count()):\n",
    "\n",
    "    # If the number of rankedNgrams coming in is zero then\n",
    "    # just return without doing anything\n",
    "    numNgrams = len(rankedNgrams)\n",
    "    if numNgrams == 0:\n",
    "        return\n",
    "\n",
    "    # This function will consider at most maxRewrite \n",
    "    # new phrases to be added into the learned phrase \n",
    "    # list as specified by the calling function\n",
    "    maxRewrite=maxPhrasesToAdd\n",
    "\n",
    "    # If the remaining number of proposed ngram phrases is less \n",
    "    # than the max allowed, then reset maxRewrite to the size of \n",
    "    # the proposed ngram phrases list\n",
    "    if numNgrams < maxRewrite:\n",
    "        maxRewrite = numNgrams\n",
    "\n",
    "    # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "    leftConflictHash = {}\n",
    "    rightConflictHash = {}\n",
    "    \n",
    "    # Create an empty hash table collecting the set of rewrite rules\n",
    "    # to be applied during this iteration of phrase learning\n",
    "    ngramRewriteHash = {}\n",
    "    \n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "\n",
    "    # Initialize some bookkeeping variables\n",
    "    numLines = len(textData)  \n",
    "    numPhrasesAdded = 0\n",
    "    numConsidered = 0\n",
    "    lastSkippedNgram = \"\"\n",
    "    lastAddedNgram = \"\"\n",
    "  \n",
    "    # Collect list of up to maxRewrite ngram phrase rewrites\n",
    "    stop = False\n",
    "    index = 0\n",
    "    while not stop:\n",
    "\n",
    "        # Get the next phrase to consider adding to the phrase list\n",
    "        inputNgram = rankedNgrams[index]\n",
    "\n",
    "        # Create the output compound word version of the phrase\n",
    "        # The extra space is added to make the regex rewrite easier\n",
    "        outputNgram = \" \" + regexSpace.sub(\"_\",inputNgram)\n",
    "\n",
    "        # Count the total number of words in the proposed phrase\n",
    "        numWords = len(outputNgram.split(\"_\"))\n",
    "\n",
    "        # Only add phrases that don't exceed the max phrase length\n",
    "        if (numWords <= maxPhraseLength):\n",
    "    \n",
    "            # Keep count of phrases considered for inclusion during this iteration\n",
    "            numConsidered += 1\n",
    "\n",
    "            # Extract the left and right words in the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = inputNgram.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[-1]\n",
    "\n",
    "            # Skip any ngram phrases that conflict with earlier phrases added\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if (leftWord in leftConflictHash) or (rightWord in rightConflictHash): \n",
    "                if verbose: \n",
    "                    print (\"(%d) Skipping (context conflict): %s\" % (numConsidered,inputNgram))\n",
    "                lastSkippedNgram = inputNgram\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                if verbose:\n",
    "                    print (\"(%d) Adding: %s\" % (numConsidered,inputNgram))\n",
    "                ngramRewriteHash[\" \" + inputNgram] = outputNgram\n",
    "                learnedPhrases.append(inputNgram) \n",
    "                lastAddedNgram = inputNgram\n",
    "                numPhrasesAdded += 1\n",
    "            \n",
    "            # Keep track of all context words that might conflict with upcoming\n",
    "            # propose phrases (even when phrases are skipped instead of added)\n",
    "            leftConflictHash[rightWord] = 1\n",
    "            rightConflictHash[leftWord] = 1\n",
    "\n",
    "            # Stop when we've considered the maximum number of phrases per iteration\n",
    "            if ( numConsidered >= maxRewrite ):\n",
    "                stop = True\n",
    "            \n",
    "        # Increment to next phrase\n",
    "        index += 1\n",
    "    \n",
    "        # Stop if we've reached the end of the ranked ngram list\n",
    "        if index >= len(rankedNgrams):\n",
    "            stop = True\n",
    "    \n",
    "    # Now do the phrase rewrites over the entire set of text data\n",
    "    # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "    ngramRegex = re.compile(r'%s(?= )' % \"(?= )|\".join(map(re.escape, ngramRewriteHash.keys())))\n",
    "    \n",
    "    # Get the number of CPU to run the tasks\n",
    "    if numWorkers > cpu_count() or numWorkers <= 0:\n",
    "        worker = cpu_count()\n",
    "    else:\n",
    "        worker = numWorkers\n",
    "    if verbose:\n",
    "        print(\"Worker size = %d\" % worker)\n",
    "        \n",
    "    # Get the batch size for each execution job\n",
    "    # The very last job executor may receive more lines of data\n",
    "    batch_size = int(numLines/worker)\n",
    "    batchIndexes = range(0, numLines, batch_size)\n",
    "    \n",
    "    batch_returns = [[]] * worker\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=worker) as executor:\n",
    "        jobs = set()\n",
    "        \n",
    "        # Map the task into multiple batch executions\n",
    "        if platform.system() == \"Linux\" or platform.system() == \"Darwin\":\n",
    "            for idx in range(worker):\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(executor.submit(phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]: ], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "                else:\n",
    "                    jobs.add(executor.submit(phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "        else:\n",
    "            from misc import winprocess\n",
    "            \n",
    "            for idx in range(worker):\n",
    "                if idx == (worker-1):\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                             phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]: ], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "                else:\n",
    "                    jobs.add(winprocess.submit(executor,\n",
    "                                             phraseRewriteJob, \n",
    "                                             ngramRegex, \n",
    "                                             textData[batchIndexes[idx]:(batchIndexes[idx]+batch_size)], \n",
    "                                             ngramRewriteHash, \n",
    "                                             idx,\n",
    "                                             verbose))\n",
    "        \n",
    "        textData.clear()\n",
    "        \n",
    "        # Get results from batch executions\n",
    "        for job in concurrent.futures.as_completed(jobs):\n",
    "            try:\n",
    "                ret, idx = job.result()\n",
    "            except Exception as e:\n",
    "                print(\"Generated an exception while trying to get result from a batch: %s\" % e)\n",
    "            else:\n",
    "                batch_returns[idx] = ret\n",
    "        textData += sum(batch_returns, [])\n",
    "     \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the full iterative phrase learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseLearning(textData,learnedPhrases,learningSettings):\n",
    "    \n",
    "    stop = False\n",
    "    iterNum = 0\n",
    "\n",
    "    # Get the learning parameters from the structure passed in by the calling function\n",
    "    maxNumPhrases = learningSettings.maxNumPhrases\n",
    "    maxPhraseLength = learningSettings.maxPhraseLength\n",
    "    functionwordHash = learningSettings.functionwordHash\n",
    "    blacklistHash = learningSettings.blacklistHash\n",
    "    verbose = learningSettings.verbose\n",
    "    minCount = learningSettings.minInstanceCount\n",
    "    \n",
    "    # Start timing the process\n",
    "    functionStartTime = time.clock()\n",
    "    \n",
    "    numPhrasesLearned = len(learnedPhrases)\n",
    "    print (\"Start phrase learning with %d phrases of %d phrases learned\" % (numPhrasesLearned,maxNumPhrases))\n",
    "\n",
    "    while not stop:\n",
    "        iterNum += 1\n",
    "                \n",
    "        # Start timing this iteration\n",
    "        startTime = time.clock()\n",
    " \n",
    "        # Collect ngram stats\n",
    "        ngramStats = ComputeNgramStats(textData, functionwordHash, blacklistHash, cpu_count(), verbose)\n",
    "\n",
    "        # Uncomment this for more detailed timing info\n",
    "        countTime = time.clock()\n",
    "        elapsedTime = countTime - startTime\n",
    "        print (\"--- Counting time: %.2f seconds\" % elapsedTime)\n",
    "        \n",
    "        # Rank ngrams\n",
    "        rankedNgrams = RankNgrams(ngramStats,functionwordHash,minCount)\n",
    "        \n",
    "        # Uncomment this for more detailed timing info\n",
    "        rankTime = time.clock()\n",
    "        elapsedTime = rankTime - countTime\n",
    "        print (\"--- Ranking time: %.2f seconds\" % elapsedTime)\n",
    "        \n",
    "        \n",
    "        # Incorporate top ranked phrases into phrase list\n",
    "        # and rewrite the text to use these phrases\n",
    "        if len(rankedNgrams) > 0:\n",
    "            maxPhrasesToAdd = maxNumPhrases - numPhrasesLearned\n",
    "            if maxPhrasesToAdd > learningSettings.maxPhrasesPerIter:\n",
    "                maxPhrasesToAdd = learningSettings.maxPhrasesPerIter\n",
    "            ApplyPhraseRewrites(rankedNgrams, textData, learnedPhrases, maxPhrasesToAdd, \n",
    "                                maxPhraseLength, verbose, cpu_count())\n",
    "            numPhrasesAdded = len(learnedPhrases) - numPhrasesLearned\n",
    "        else:\n",
    "            stop = True\n",
    "            \n",
    "        # Uncomment this for more detailed timing info\n",
    "        rewriteTime = time.clock()\n",
    "        elapsedTime = rewriteTime - rankTime\n",
    "        print (\"--- Rewriting time: %.2f seconds\" % elapsedTime)\n",
    "           \n",
    "        # Garbage collect\n",
    "        ngramStats = None\n",
    "        rankedNgrams = None\n",
    "        gc.collect();\n",
    "               \n",
    "        elapsedTime = time.clock() - startTime\n",
    "\n",
    "        numPhrasesLearned = len(learnedPhrases)\n",
    "        print (\"Iteration %d: Added %d new phrases in %.2f seconds (Learned %d of max %d)\" % \n",
    "               (iterNum,numPhrasesAdded,elapsedTime,numPhrasesLearned,maxNumPhrases))\n",
    "        \n",
    "        if numPhrasesAdded >= maxPhrasesToAdd or numPhrasesAdded == 0:\n",
    "            stop = True\n",
    "        \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i] = regexSpacePadding.sub(\"\",textData[i])\n",
    "    \n",
    "    gc.collect()\n",
    " \n",
    "    elapsedTime = time.clock() - functionStartTime\n",
    "    elapsedTimeHours = elapsedTime/3600.0;\n",
    "    print (\"*** Phrase learning completed in %.2f hours ***\" % elapsedTimeHours) \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Main top level execution of phrase learning functionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** The phrase learning step is implemented with multiprocessing. However, more CPU cores do NOT mean a faster execution time. In our tests, the performance is not improved with more than eight cores due to the overhead of multiprocessing. It took about two and a half hours to learn 25,000 phrases on a machine with eight cores (3.6 GHz). \n",
    "\n",
    "> If you just need to run the code and see how it works, change the variable `learningSettings.maxNumPhrases` in the cell below to a small number, by default it will try to learn 25,000 phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 25000 phrases learned\n",
      "--- Counting time: 28.60 seconds\n",
      "--- Ranking time: 3.49 seconds\n",
      "--- Rewriting time: 17.71 seconds\n",
      "Iteration 1: Added 243 new phrases in 50.09 seconds (Learned 243 of max 25000)\n",
      "--- Counting time: 26.93 seconds\n",
      "--- Ranking time: 3.54 seconds\n",
      "--- Rewriting time: 16.67 seconds\n",
      "Iteration 2: Added 220 new phrases in 47.53 seconds (Learned 463 of max 25000)\n",
      "--- Counting time: 28.75 seconds\n",
      "--- Ranking time: 3.77 seconds\n",
      "--- Rewriting time: 16.69 seconds\n",
      "Iteration 3: Added 202 new phrases in 49.57 seconds (Learned 665 of max 25000)\n",
      "--- Counting time: 28.81 seconds\n",
      "--- Ranking time: 3.86 seconds\n",
      "--- Rewriting time: 15.27 seconds\n",
      "Iteration 4: Added 188 new phrases in 48.33 seconds (Learned 853 of max 25000)\n",
      "--- Counting time: 28.92 seconds\n",
      "--- Ranking time: 4.19 seconds\n",
      "--- Rewriting time: 14.27 seconds\n",
      "Iteration 5: Added 169 new phrases in 47.75 seconds (Learned 1022 of max 25000)\n",
      "--- Counting time: 28.38 seconds\n",
      "--- Ranking time: 3.59 seconds\n",
      "--- Rewriting time: 11.16 seconds\n",
      "Iteration 6: Added 147 new phrases in 43.51 seconds (Learned 1169 of max 25000)\n",
      "--- Counting time: 26.28 seconds\n",
      "--- Ranking time: 3.67 seconds\n",
      "--- Rewriting time: 9.87 seconds\n",
      "Iteration 7: Added 124 new phrases in 40.22 seconds (Learned 1293 of max 25000)\n",
      "--- Counting time: 26.58 seconds\n",
      "--- Ranking time: 3.68 seconds\n",
      "--- Rewriting time: 11.25 seconds\n",
      "Iteration 8: Added 135 new phrases in 41.90 seconds (Learned 1428 of max 25000)\n",
      "--- Counting time: 26.77 seconds\n",
      "--- Ranking time: 3.61 seconds\n",
      "--- Rewriting time: 11.00 seconds\n",
      "Iteration 9: Added 146 new phrases in 41.78 seconds (Learned 1574 of max 25000)\n",
      "--- Counting time: 26.15 seconds\n",
      "--- Ranking time: 3.62 seconds\n",
      "--- Rewriting time: 12.23 seconds\n",
      "Iteration 10: Added 163 new phrases in 42.41 seconds (Learned 1737 of max 25000)\n",
      "--- Counting time: 26.18 seconds\n",
      "--- Ranking time: 3.64 seconds\n",
      "--- Rewriting time: 12.56 seconds\n",
      "Iteration 11: Added 158 new phrases in 42.78 seconds (Learned 1895 of max 25000)\n",
      "--- Counting time: 26.50 seconds\n",
      "--- Ranking time: 3.70 seconds\n",
      "--- Rewriting time: 11.92 seconds\n",
      "Iteration 12: Added 164 new phrases in 42.54 seconds (Learned 2059 of max 25000)\n",
      "--- Counting time: 25.89 seconds\n",
      "--- Ranking time: 3.63 seconds\n",
      "--- Rewriting time: 12.45 seconds\n",
      "Iteration 13: Added 168 new phrases in 42.39 seconds (Learned 2227 of max 25000)\n",
      "--- Counting time: 28.07 seconds\n",
      "--- Ranking time: 3.66 seconds\n",
      "--- Rewriting time: 11.21 seconds\n",
      "Iteration 14: Added 148 new phrases in 43.36 seconds (Learned 2375 of max 25000)\n",
      "--- Counting time: 26.07 seconds\n",
      "--- Ranking time: 3.67 seconds\n",
      "--- Rewriting time: 10.64 seconds\n",
      "Iteration 15: Added 139 new phrases in 40.79 seconds (Learned 2514 of max 25000)\n",
      "--- Counting time: 25.94 seconds\n",
      "--- Ranking time: 3.65 seconds\n",
      "--- Rewriting time: 10.91 seconds\n",
      "Iteration 16: Added 141 new phrases in 40.92 seconds (Learned 2655 of max 25000)\n",
      "--- Counting time: 27.66 seconds\n",
      "--- Ranking time: 3.75 seconds\n",
      "--- Rewriting time: 10.48 seconds\n",
      "Iteration 17: Added 130 new phrases in 42.31 seconds (Learned 2785 of max 25000)\n",
      "--- Counting time: 26.04 seconds\n",
      "--- Ranking time: 3.68 seconds\n",
      "--- Rewriting time: 9.87 seconds\n",
      "Iteration 18: Added 129 new phrases in 40.02 seconds (Learned 2914 of max 25000)\n",
      "--- Counting time: 25.93 seconds\n",
      "--- Ranking time: 3.66 seconds\n",
      "--- Rewriting time: 10.50 seconds\n",
      "Iteration 19: Added 136 new phrases in 40.52 seconds (Learned 3050 of max 25000)\n",
      "--- Counting time: 27.95 seconds\n",
      "--- Ranking time: 3.74 seconds\n",
      "--- Rewriting time: 9.92 seconds\n",
      "Iteration 20: Added 122 new phrases in 42.02 seconds (Learned 3172 of max 25000)\n",
      "--- Counting time: 25.88 seconds\n",
      "--- Ranking time: 3.71 seconds\n",
      "--- Rewriting time: 9.73 seconds\n",
      "Iteration 21: Added 120 new phrases in 39.74 seconds (Learned 3292 of max 25000)\n",
      "--- Counting time: 25.85 seconds\n",
      "--- Ranking time: 3.73 seconds\n",
      "--- Rewriting time: 9.69 seconds\n",
      "Iteration 22: Added 119 new phrases in 39.70 seconds (Learned 3411 of max 25000)\n",
      "--- Counting time: 27.44 seconds\n",
      "--- Ranking time: 3.88 seconds\n",
      "--- Rewriting time: 9.19 seconds\n",
      "Iteration 23: Added 107 new phrases in 40.94 seconds (Learned 3518 of max 25000)\n",
      "--- Counting time: 25.83 seconds\n",
      "--- Ranking time: 3.68 seconds\n",
      "--- Rewriting time: 9.03 seconds\n",
      "Iteration 24: Added 109 new phrases in 38.96 seconds (Learned 3627 of max 25000)\n",
      "--- Counting time: 25.79 seconds\n",
      "--- Ranking time: 3.71 seconds\n",
      "--- Rewriting time: 9.01 seconds\n",
      "Iteration 25: Added 100 new phrases in 38.94 seconds (Learned 3727 of max 25000)\n",
      "--- Counting time: 27.28 seconds\n",
      "--- Ranking time: 3.85 seconds\n",
      "--- Rewriting time: 10.02 seconds\n",
      "Iteration 26: Added 119 new phrases in 41.58 seconds (Learned 3846 of max 25000)\n",
      "--- Counting time: 25.76 seconds\n",
      "--- Ranking time: 3.69 seconds\n",
      "--- Rewriting time: 10.71 seconds\n",
      "Iteration 27: Added 143 new phrases in 40.60 seconds (Learned 3989 of max 25000)\n",
      "--- Counting time: 25.68 seconds\n",
      "--- Ranking time: 3.72 seconds\n",
      "--- Rewriting time: 13.02 seconds\n",
      "Iteration 28: Added 164 new phrases in 42.85 seconds (Learned 4153 of max 25000)\n",
      "--- Counting time: 27.61 seconds\n",
      "--- Ranking time: 3.79 seconds\n",
      "--- Rewriting time: 11.99 seconds\n",
      "Iteration 29: Added 159 new phrases in 43.83 seconds (Learned 4312 of max 25000)\n",
      "--- Counting time: 25.96 seconds\n",
      "--- Ranking time: 3.75 seconds\n",
      "--- Rewriting time: 11.33 seconds\n",
      "Iteration 30: Added 154 new phrases in 41.47 seconds (Learned 4466 of max 25000)\n",
      "--- Counting time: 25.89 seconds\n",
      "--- Ranking time: 3.90 seconds\n",
      "--- Rewriting time: 12.82 seconds\n",
      "Iteration 31: Added 157 new phrases in 43.06 seconds (Learned 4623 of max 25000)\n",
      "--- Counting time: 27.23 seconds\n",
      "--- Ranking time: 3.78 seconds\n",
      "--- Rewriting time: 10.91 seconds\n",
      "Iteration 32: Added 149 new phrases in 42.36 seconds (Learned 4772 of max 25000)\n",
      "--- Counting time: 25.67 seconds\n",
      "--- Ranking time: 3.75 seconds\n",
      "--- Rewriting time: 12.56 seconds\n",
      "Iteration 33: Added 174 new phrases in 42.42 seconds (Learned 4946 of max 25000)\n",
      "--- Counting time: 26.05 seconds\n",
      "--- Ranking time: 3.88 seconds\n",
      "--- Rewriting time: 13.71 seconds\n",
      "Iteration 34: Added 179 new phrases in 44.09 seconds (Learned 5125 of max 25000)\n",
      "--- Counting time: 26.43 seconds\n",
      "--- Ranking time: 3.78 seconds\n",
      "--- Rewriting time: 12.15 seconds\n",
      "Iteration 35: Added 167 new phrases in 42.80 seconds (Learned 5292 of max 25000)\n",
      "--- Counting time: 25.81 seconds\n",
      "--- Ranking time: 3.78 seconds\n",
      "--- Rewriting time: 12.06 seconds\n",
      "Iteration 36: Added 163 new phrases in 42.09 seconds (Learned 5455 of max 25000)\n",
      "--- Counting time: 25.89 seconds\n",
      "--- Ranking time: 3.89 seconds\n",
      "--- Rewriting time: 11.09 seconds\n",
      "Iteration 37: Added 132 new phrases in 41.30 seconds (Learned 5587 of max 25000)\n",
      "--- Counting time: 26.79 seconds\n",
      "--- Ranking time: 3.82 seconds\n",
      "--- Rewriting time: 10.33 seconds\n",
      "Iteration 38: Added 134 new phrases in 41.37 seconds (Learned 5721 of max 25000)\n",
      "--- Counting time: 25.69 seconds\n",
      "--- Ranking time: 3.80 seconds\n",
      "--- Rewriting time: 9.65 seconds\n",
      "Iteration 39: Added 122 new phrases in 39.57 seconds (Learned 5843 of max 25000)\n",
      "--- Counting time: 26.15 seconds\n",
      "--- Ranking time: 3.80 seconds\n",
      "--- Rewriting time: 11.04 seconds\n",
      "Iteration 40: Added 131 new phrases in 41.43 seconds (Learned 5974 of max 25000)\n",
      "--- Counting time: 26.57 seconds\n",
      "--- Ranking time: 3.81 seconds\n",
      "--- Rewriting time: 9.99 seconds\n",
      "Iteration 41: Added 127 new phrases in 40.80 seconds (Learned 6101 of max 25000)\n",
      "--- Counting time: 25.71 seconds\n",
      "--- Ranking time: 3.80 seconds\n",
      "--- Rewriting time: 9.17 seconds\n",
      "Iteration 42: Added 112 new phrases in 39.12 seconds (Learned 6213 of max 25000)\n",
      "--- Counting time: 25.83 seconds\n",
      "--- Ranking time: 3.84 seconds\n",
      "--- Rewriting time: 9.87 seconds\n",
      "Iteration 43: Added 108 new phrases in 39.99 seconds (Learned 6321 of max 25000)\n",
      "--- Counting time: 26.70 seconds\n",
      "--- Ranking time: 3.81 seconds\n",
      "--- Rewriting time: 9.02 seconds\n",
      "Iteration 44: Added 109 new phrases in 39.97 seconds (Learned 6430 of max 25000)\n",
      "--- Counting time: 25.50 seconds\n",
      "--- Ranking time: 3.80 seconds\n",
      "--- Rewriting time: 8.61 seconds\n",
      "Iteration 45: Added 98 new phrases in 38.33 seconds (Learned 6528 of max 25000)\n",
      "--- Counting time: 25.83 seconds\n",
      "--- Ranking time: 3.80 seconds\n",
      "--- Rewriting time: 9.01 seconds\n",
      "Iteration 46: Added 93 new phrases in 39.08 seconds (Learned 6621 of max 25000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counting time: 26.62 seconds\n",
      "--- Ranking time: 3.83 seconds\n",
      "--- Rewriting time: 9.40 seconds\n",
      "Iteration 47: Added 118 new phrases in 40.30 seconds (Learned 6739 of max 25000)\n",
      "--- Counting time: 25.52 seconds\n",
      "--- Ranking time: 3.82 seconds\n",
      "--- Rewriting time: 11.04 seconds\n",
      "Iteration 48: Added 150 new phrases in 40.83 seconds (Learned 6889 of max 25000)\n",
      "--- Counting time: 25.78 seconds\n",
      "--- Ranking time: 3.95 seconds\n",
      "--- Rewriting time: 13.70 seconds\n",
      "Iteration 49: Added 173 new phrases in 43.88 seconds (Learned 7062 of max 25000)\n",
      "--- Counting time: 26.04 seconds\n",
      "--- Ranking time: 3.87 seconds\n",
      "--- Rewriting time: 13.43 seconds\n",
      "Iteration 50: Added 186 new phrases in 43.79 seconds (Learned 7248 of max 25000)\n",
      "--- Counting time: 25.48 seconds\n",
      "--- Ranking time: 3.84 seconds\n",
      "--- Rewriting time: 14.41 seconds\n",
      "Iteration 51: Added 205 new phrases in 44.19 seconds (Learned 7453 of max 25000)\n",
      "--- Counting time: 26.94 seconds\n",
      "--- Ranking time: 3.85 seconds\n",
      "--- Rewriting time: 14.62 seconds\n",
      "Iteration 52: Added 205 new phrases in 45.87 seconds (Learned 7658 of max 25000)\n",
      "--- Counting time: 25.46 seconds\n",
      "--- Ranking time: 3.86 seconds\n",
      "--- Rewriting time: 14.29 seconds\n",
      "Iteration 53: Added 207 new phrases in 44.04 seconds (Learned 7865 of max 25000)\n",
      "--- Counting time: 25.57 seconds\n",
      "--- Ranking time: 3.83 seconds\n",
      "--- Rewriting time: 16.40 seconds\n",
      "Iteration 54: Added 217 new phrases in 46.26 seconds (Learned 8082 of max 25000)\n",
      "--- Counting time: 26.25 seconds\n",
      "--- Ranking time: 3.87 seconds\n",
      "--- Rewriting time: 16.34 seconds\n",
      "Iteration 55: Added 240 new phrases in 46.92 seconds (Learned 8322 of max 25000)\n",
      "--- Counting time: 25.61 seconds\n",
      "--- Ranking time: 3.86 seconds\n",
      "--- Rewriting time: 16.33 seconds\n",
      "Iteration 56: Added 244 new phrases in 46.24 seconds (Learned 8566 of max 25000)\n",
      "--- Counting time: 25.70 seconds\n",
      "--- Ranking time: 3.99 seconds\n",
      "--- Rewriting time: 18.40 seconds\n",
      "Iteration 57: Added 243 new phrases in 48.53 seconds (Learned 8809 of max 25000)\n",
      "--- Counting time: 25.68 seconds\n",
      "--- Ranking time: 3.86 seconds\n",
      "--- Rewriting time: 15.88 seconds\n",
      "Iteration 58: Added 233 new phrases in 45.88 seconds (Learned 9042 of max 25000)\n",
      "--- Counting time: 25.51 seconds\n",
      "--- Ranking time: 3.95 seconds\n",
      "--- Rewriting time: 16.96 seconds\n",
      "Iteration 59: Added 232 new phrases in 46.89 seconds (Learned 9274 of max 25000)\n",
      "--- Counting time: 27.17 seconds\n",
      "--- Ranking time: 3.95 seconds\n",
      "--- Rewriting time: 15.01 seconds\n",
      "Iteration 60: Added 217 new phrases in 46.58 seconds (Learned 9491 of max 25000)\n",
      "--- Counting time: 25.42 seconds\n",
      "--- Ranking time: 3.86 seconds\n",
      "--- Rewriting time: 14.99 seconds\n",
      "Iteration 61: Added 217 new phrases in 44.72 seconds (Learned 9708 of max 25000)\n",
      "--- Counting time: 25.56 seconds\n",
      "--- Ranking time: 3.86 seconds\n",
      "--- Rewriting time: 15.44 seconds\n",
      "Iteration 62: Added 196 new phrases in 45.31 seconds (Learned 9904 of max 25000)\n",
      "--- Counting time: 25.93 seconds\n",
      "--- Ranking time: 3.93 seconds\n",
      "--- Rewriting time: 14.26 seconds\n",
      "Iteration 63: Added 201 new phrases in 44.56 seconds (Learned 10105 of max 25000)\n",
      "--- Counting time: 25.47 seconds\n",
      "--- Ranking time: 3.91 seconds\n",
      "--- Rewriting time: 13.71 seconds\n",
      "Iteration 64: Added 196 new phrases in 43.56 seconds (Learned 10301 of max 25000)\n",
      "--- Counting time: 26.86 seconds\n",
      "--- Ranking time: 3.94 seconds\n",
      "--- Rewriting time: 13.39 seconds\n",
      "Iteration 65: Added 188 new phrases in 44.65 seconds (Learned 10489 of max 25000)\n",
      "--- Counting time: 25.52 seconds\n",
      "--- Ranking time: 3.91 seconds\n",
      "--- Rewriting time: 15.58 seconds\n",
      "Iteration 66: Added 228 new phrases in 45.48 seconds (Learned 10717 of max 25000)\n",
      "--- Counting time: 25.45 seconds\n",
      "--- Ranking time: 3.99 seconds\n",
      "--- Rewriting time: 16.74 seconds\n",
      "Iteration 67: Added 231 new phrases in 46.65 seconds (Learned 10948 of max 25000)\n",
      "--- Counting time: 27.39 seconds\n",
      "--- Ranking time: 3.90 seconds\n",
      "--- Rewriting time: 15.22 seconds\n",
      "Iteration 68: Added 222 new phrases in 46.96 seconds (Learned 11170 of max 25000)\n",
      "--- Counting time: 26.24 seconds\n",
      "--- Ranking time: 4.10 seconds\n",
      "--- Rewriting time: 18.03 seconds\n",
      "Iteration 69: Added 253 new phrases in 48.92 seconds (Learned 11423 of max 25000)\n",
      "--- Counting time: 29.05 seconds\n",
      "--- Ranking time: 4.39 seconds\n",
      "--- Rewriting time: 19.54 seconds\n",
      "Iteration 70: Added 258 new phrases in 53.47 seconds (Learned 11681 of max 25000)\n",
      "--- Counting time: 26.80 seconds\n",
      "--- Ranking time: 4.22 seconds\n",
      "--- Rewriting time: 18.29 seconds\n",
      "Iteration 71: Added 257 new phrases in 49.79 seconds (Learned 11938 of max 25000)\n",
      "--- Counting time: 27.50 seconds\n",
      "--- Ranking time: 4.47 seconds\n",
      "--- Rewriting time: 20.16 seconds\n",
      "Iteration 72: Added 269 new phrases in 52.62 seconds (Learned 12207 of max 25000)\n",
      "--- Counting time: 27.24 seconds\n",
      "--- Ranking time: 4.27 seconds\n",
      "--- Rewriting time: 18.10 seconds\n",
      "Iteration 73: Added 260 new phrases in 50.10 seconds (Learned 12467 of max 25000)\n",
      "--- Counting time: 26.51 seconds\n",
      "--- Ranking time: 4.24 seconds\n",
      "--- Rewriting time: 18.87 seconds\n",
      "Iteration 74: Added 276 new phrases in 50.11 seconds (Learned 12743 of max 25000)\n",
      "--- Counting time: 28.80 seconds\n",
      "--- Ranking time: 4.27 seconds\n",
      "--- Rewriting time: 19.37 seconds\n",
      "Iteration 75: Added 289 new phrases in 52.94 seconds (Learned 13032 of max 25000)\n",
      "--- Counting time: 27.22 seconds\n",
      "--- Ranking time: 4.17 seconds\n",
      "--- Rewriting time: 20.26 seconds\n",
      "Iteration 76: Added 292 new phrases in 52.15 seconds (Learned 13324 of max 25000)\n",
      "--- Counting time: 27.67 seconds\n",
      "--- Ranking time: 4.37 seconds\n",
      "--- Rewriting time: 20.35 seconds\n",
      "Iteration 77: Added 273 new phrases in 52.89 seconds (Learned 13597 of max 25000)\n",
      "--- Counting time: 26.97 seconds\n",
      "--- Ranking time: 4.21 seconds\n",
      "--- Rewriting time: 20.05 seconds\n",
      "Iteration 78: Added 290 new phrases in 51.76 seconds (Learned 13887 of max 25000)\n",
      "--- Counting time: 27.41 seconds\n",
      "--- Ranking time: 4.21 seconds\n",
      "--- Rewriting time: 20.92 seconds\n",
      "Iteration 79: Added 287 new phrases in 53.02 seconds (Learned 14174 of max 25000)\n",
      "--- Counting time: 27.52 seconds\n",
      "--- Ranking time: 4.22 seconds\n",
      "--- Rewriting time: 20.07 seconds\n",
      "Iteration 80: Added 299 new phrases in 52.31 seconds (Learned 14473 of max 25000)\n",
      "--- Counting time: 27.59 seconds\n",
      "--- Ranking time: 4.22 seconds\n",
      "--- Rewriting time: 22.35 seconds\n",
      "Iteration 81: Added 292 new phrases in 54.66 seconds (Learned 14765 of max 25000)\n",
      "--- Counting time: 29.16 seconds\n",
      "--- Ranking time: 4.14 seconds\n",
      "--- Rewriting time: 18.90 seconds\n",
      "Iteration 82: Added 292 new phrases in 52.70 seconds (Learned 15057 of max 25000)\n",
      "--- Counting time: 26.59 seconds\n",
      "--- Ranking time: 3.96 seconds\n",
      "--- Rewriting time: 19.12 seconds\n",
      "Iteration 83: Added 304 new phrases in 50.14 seconds (Learned 15361 of max 25000)\n",
      "--- Counting time: 26.78 seconds\n",
      "--- Ranking time: 4.05 seconds\n",
      "--- Rewriting time: 18.84 seconds\n",
      "Iteration 84: Added 293 new phrases in 50.12 seconds (Learned 15654 of max 25000)\n",
      "--- Counting time: 25.43 seconds\n",
      "--- Ranking time: 4.03 seconds\n",
      "--- Rewriting time: 18.29 seconds\n",
      "Iteration 85: Added 285 new phrases in 48.24 seconds (Learned 15939 of max 25000)\n",
      "--- Counting time: 25.55 seconds\n",
      "--- Ranking time: 4.10 seconds\n",
      "--- Rewriting time: 19.74 seconds\n",
      "Iteration 86: Added 282 new phrases in 49.88 seconds (Learned 16221 of max 25000)\n",
      "--- Counting time: 25.44 seconds\n",
      "--- Ranking time: 4.03 seconds\n",
      "--- Rewriting time: 17.83 seconds\n",
      "Iteration 87: Added 278 new phrases in 47.79 seconds (Learned 16499 of max 25000)\n",
      "--- Counting time: 25.36 seconds\n",
      "--- Ranking time: 4.00 seconds\n",
      "--- Rewriting time: 16.77 seconds\n",
      "Iteration 88: Added 253 new phrases in 46.60 seconds (Learned 16752 of max 25000)\n",
      "--- Counting time: 27.27 seconds\n",
      "--- Ranking time: 4.11 seconds\n",
      "--- Rewriting time: 16.72 seconds\n",
      "Iteration 89: Added 246 new phrases in 48.56 seconds (Learned 16998 of max 25000)\n",
      "--- Counting time: 25.36 seconds\n",
      "--- Ranking time: 3.99 seconds\n",
      "--- Rewriting time: 15.39 seconds\n",
      "Iteration 90: Added 231 new phrases in 45.21 seconds (Learned 17229 of max 25000)\n",
      "--- Counting time: 25.31 seconds\n",
      "--- Ranking time: 4.02 seconds\n",
      "--- Rewriting time: 18.73 seconds\n",
      "Iteration 91: Added 269 new phrases in 48.51 seconds (Learned 17498 of max 25000)\n",
      "--- Counting time: 25.69 seconds\n",
      "--- Ranking time: 3.99 seconds\n",
      "--- Rewriting time: 17.30 seconds\n",
      "Iteration 92: Added 262 new phrases in 47.46 seconds (Learned 17760 of max 25000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counting time: 25.44 seconds\n",
      "--- Ranking time: 4.01 seconds\n",
      "--- Rewriting time: 17.88 seconds\n",
      "Iteration 93: Added 272 new phrases in 47.80 seconds (Learned 18032 of max 25000)\n",
      "--- Counting time: 26.81 seconds\n",
      "--- Ranking time: 4.16 seconds\n",
      "--- Rewriting time: 18.17 seconds\n",
      "Iteration 94: Added 286 new phrases in 49.63 seconds (Learned 18318 of max 25000)\n",
      "--- Counting time: 25.38 seconds\n",
      "--- Ranking time: 4.03 seconds\n",
      "--- Rewriting time: 18.44 seconds\n",
      "Iteration 95: Added 295 new phrases in 48.33 seconds (Learned 18613 of max 25000)\n",
      "--- Counting time: 25.57 seconds\n",
      "--- Ranking time: 4.10 seconds\n",
      "--- Rewriting time: 21.35 seconds\n",
      "Iteration 96: Added 311 new phrases in 51.50 seconds (Learned 18924 of max 25000)\n",
      "--- Counting time: 25.71 seconds\n",
      "--- Ranking time: 4.10 seconds\n",
      "--- Rewriting time: 19.41 seconds\n",
      "Iteration 97: Added 311 new phrases in 49.72 seconds (Learned 19235 of max 25000)\n",
      "--- Counting time: 26.05 seconds\n",
      "--- Ranking time: 4.01 seconds\n",
      "--- Rewriting time: 19.70 seconds\n",
      "Iteration 98: Added 324 new phrases in 50.23 seconds (Learned 19559 of max 25000)\n",
      "--- Counting time: 27.43 seconds\n",
      "--- Ranking time: 4.09 seconds\n",
      "--- Rewriting time: 20.73 seconds\n",
      "Iteration 99: Added 336 new phrases in 52.74 seconds (Learned 19895 of max 25000)\n",
      "--- Counting time: 25.20 seconds\n",
      "--- Ranking time: 3.93 seconds\n",
      "--- Rewriting time: 20.63 seconds\n",
      "Iteration 100: Added 340 new phrases in 50.24 seconds (Learned 20235 of max 25000)\n",
      "--- Counting time: 25.31 seconds\n",
      "--- Ranking time: 4.00 seconds\n",
      "--- Rewriting time: 22.37 seconds\n",
      "Iteration 101: Added 332 new phrases in 52.16 seconds (Learned 20567 of max 25000)\n",
      "--- Counting time: 25.39 seconds\n",
      "--- Ranking time: 3.96 seconds\n",
      "--- Rewriting time: 20.51 seconds\n",
      "Iteration 102: Added 338 new phrases in 50.33 seconds (Learned 20905 of max 25000)\n",
      "--- Counting time: 25.48 seconds\n",
      "--- Ranking time: 4.14 seconds\n",
      "--- Rewriting time: 22.20 seconds\n",
      "Iteration 103: Added 326 new phrases in 52.30 seconds (Learned 21231 of max 25000)\n",
      "--- Counting time: 25.35 seconds\n",
      "--- Ranking time: 4.07 seconds\n",
      "--- Rewriting time: 19.06 seconds\n",
      "Iteration 104: Added 313 new phrases in 48.96 seconds (Learned 21544 of max 25000)\n",
      "--- Counting time: 25.15 seconds\n",
      "--- Ranking time: 4.02 seconds\n",
      "--- Rewriting time: 19.27 seconds\n",
      "Iteration 105: Added 316 new phrases in 48.93 seconds (Learned 21860 of max 25000)\n",
      "--- Counting time: 27.31 seconds\n",
      "--- Ranking time: 4.09 seconds\n",
      "--- Rewriting time: 18.81 seconds\n",
      "Iteration 106: Added 300 new phrases in 50.71 seconds (Learned 22160 of max 25000)\n",
      "--- Counting time: 25.22 seconds\n",
      "--- Ranking time: 4.00 seconds\n",
      "--- Rewriting time: 18.45 seconds\n",
      "Iteration 107: Added 299 new phrases in 48.15 seconds (Learned 22459 of max 25000)\n",
      "--- Counting time: 25.29 seconds\n",
      "--- Ranking time: 4.04 seconds\n",
      "--- Rewriting time: 20.99 seconds\n",
      "Iteration 108: Added 302 new phrases in 50.81 seconds (Learned 22761 of max 25000)\n",
      "--- Counting time: 25.38 seconds\n",
      "--- Ranking time: 4.01 seconds\n",
      "--- Rewriting time: 18.64 seconds\n",
      "Iteration 109: Added 304 new phrases in 48.52 seconds (Learned 23065 of max 25000)\n",
      "--- Counting time: 25.07 seconds\n",
      "--- Ranking time: 3.99 seconds\n",
      "--- Rewriting time: 18.82 seconds\n",
      "Iteration 110: Added 295 new phrases in 48.36 seconds (Learned 23360 of max 25000)\n",
      "--- Counting time: 26.49 seconds\n",
      "--- Ranking time: 4.15 seconds\n",
      "--- Rewriting time: 18.85 seconds\n",
      "Iteration 111: Added 297 new phrases in 49.99 seconds (Learned 23657 of max 25000)\n",
      "--- Counting time: 27.42 seconds\n",
      "--- Ranking time: 4.43 seconds\n",
      "--- Rewriting time: 18.05 seconds\n",
      "Iteration 112: Added 276 new phrases in 50.40 seconds (Learned 23933 of max 25000)\n",
      "--- Counting time: 25.84 seconds\n",
      "--- Ranking time: 4.22 seconds\n",
      "--- Rewriting time: 20.01 seconds\n",
      "Iteration 113: Added 289 new phrases in 50.59 seconds (Learned 24222 of max 25000)\n",
      "--- Counting time: 25.74 seconds\n",
      "--- Ranking time: 4.07 seconds\n",
      "--- Rewriting time: 18.56 seconds\n",
      "Iteration 114: Added 294 new phrases in 48.86 seconds (Learned 24516 of max 25000)\n",
      "--- Counting time: 25.62 seconds\n",
      "--- Ranking time: 4.16 seconds\n",
      "--- Rewriting time: 19.29 seconds\n",
      "Iteration 115: Added 278 new phrases in 49.56 seconds (Learned 24794 of max 25000)\n",
      "--- Counting time: 26.39 seconds\n",
      "--- Ranking time: 4.06 seconds\n",
      "--- Rewriting time: 7.76 seconds\n",
      "Iteration 116: Added 67 new phrases in 38.70 seconds (Learned 24861 of max 25000)\n",
      "--- Counting time: 25.35 seconds\n",
      "--- Ranking time: 4.06 seconds\n",
      "--- Rewriting time: 6.69 seconds\n",
      "Iteration 117: Added 35 new phrases in 36.60 seconds (Learned 24896 of max 25000)\n",
      "--- Counting time: 27.89 seconds\n",
      "--- Ranking time: 4.40 seconds\n",
      "--- Rewriting time: 7.63 seconds\n",
      "Iteration 118: Added 28 new phrases in 40.54 seconds (Learned 24924 of max 25000)\n",
      "--- Counting time: 29.05 seconds\n",
      "--- Ranking time: 4.45 seconds\n",
      "--- Rewriting time: 6.61 seconds\n",
      "Iteration 119: Added 14 new phrases in 40.63 seconds (Learned 24938 of max 25000)\n",
      "--- Counting time: 27.84 seconds\n",
      "--- Ranking time: 4.56 seconds\n",
      "--- Rewriting time: 7.52 seconds\n",
      "Iteration 120: Added 43 new phrases in 40.44 seconds (Learned 24981 of max 25000)\n",
      "--- Counting time: 27.10 seconds\n",
      "--- Ranking time: 4.35 seconds\n",
      "--- Rewriting time: 6.64 seconds\n",
      "Iteration 121: Added 8 new phrases in 38.59 seconds (Learned 24989 of max 25000)\n",
      "--- Counting time: 27.64 seconds\n",
      "--- Ranking time: 4.35 seconds\n",
      "--- Rewriting time: 6.24 seconds\n",
      "Iteration 122: Added 6 new phrases in 38.73 seconds (Learned 24995 of max 25000)\n",
      "--- Counting time: 26.89 seconds\n",
      "--- Ranking time: 4.35 seconds\n",
      "--- Rewriting time: 6.14 seconds\n",
      "Iteration 123: Added 3 new phrases in 37.90 seconds (Learned 24998 of max 25000)\n",
      "--- Counting time: 26.85 seconds\n",
      "--- Ranking time: 4.37 seconds\n",
      "--- Rewriting time: 6.18 seconds\n",
      "Iteration 124: Added 2 new phrases in 37.92 seconds (Learned 25000 of max 25000)\n",
      "*** Phrase learning completed in 1.57 hours ***\n"
     ]
    }
   ],
   "source": [
    "# Create a structure defining the settings and word lists used during the phrase learning\n",
    "learningSettings = namedtuple('learningSettings',['maxNumPhrases','maxPhrasesPerIter',\n",
    "                                                  'maxPhraseLength','minInstanceCount'\n",
    "                                                  'functionwordHash','blacklistHash','verbose'])\n",
    "\n",
    "# If true it prints out the learned phrases to stdout buffer\n",
    "# while its learning. This will generate a lot of text to stdout, \n",
    "# so best to turn this off except for testing and debugging\n",
    "learningSettings.verbose = False\n",
    "\n",
    "# Maximum number of phrases to learn\n",
    "# If you want to test the code out quickly then set this to a small\n",
    "# value (e.g. 100) and set verbose to true when running the quick test\n",
    "learningSettings.maxNumPhrases = 25000\n",
    "\n",
    "# Maximum number of phrases to learn per iteration \n",
    "# Increasing this number may speed up processing but will affect the ordering of the phrases \n",
    "# learned and good phrases could be by-passed if the maxNumPhrases is set to a small number\n",
    "learningSettings.maxPhrasesPerIter = 500\n",
    "\n",
    "# Maximum number of words allowed in the learned phrases \n",
    "learningSettings.maxPhraseLength = 7\n",
    "\n",
    "# Minimum number of times a phrase must occur in the data to \n",
    "# be considered during the phrase learning process\n",
    "learningSettings.minInstanceCount = 5\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of function words used during phrase learning\n",
    "learningSettings.functionwordHash = functionwordHash\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of black list words to be ignored during phrase learning\n",
    "learningSettings.blacklistHash = blacklistHash\n",
    "\n",
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrases = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextData = []\n",
    "for textLine in textFrame['LowercaseText']:\n",
    "    phraseTextData.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm\n",
    "if True:\n",
    "    ApplyPhraseLearning(phraseTextData, learnedPhrases, learningSettings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "Persist the learned phrases:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnedPhrasesFile = os.path.join(\"./Data\", \"CongressionalDocsLearnedPhrases.txt\")\n",
    "phraseTextDataFile = os.path.join(\"./Data\", \"CongressionalDocsPhraseTextData.txt\")\n",
    "\n",
    "writeLearnedPhrases = True\n",
    "\n",
    "if writeLearnedPhrases:\n",
    "    # Write out the learned phrases to a text file\n",
    "    fp = open(learnedPhrasesFile, 'w', encoding='utf-8')\n",
    "    for phrase in learnedPhrases:\n",
    "        fp.write(\"%s\\n\" % phrase)\n",
    "    fp.close()\n",
    "\n",
    "    # Write out the text data containing the learned phrases to a text file\n",
    "    fp = open(phraseTextDataFile, 'w', encoding='utf-8')\n",
    "    for line in phraseTextData:\n",
    "        fp.write(\"%s\\n\" % line)\n",
    "    fp.close()\n",
    "else:\n",
    "    # Read in the learned phrases from a text file\n",
    "    learnedPhrases = []\n",
    "    fp = open(learnedPhrasesFile, 'r', encoding='utf-8')\n",
    "    for line in fp:\n",
    "        learnedPhrases.append(line.strip())\n",
    "    fp.close()\n",
    "\n",
    "    # Read in the learned phrases from a text file\n",
    "    phraseTextData = []\n",
    "    fp = open(phraseTextDataFile, 'r', encoding='utf-8')\n",
    "    for line in fp:\n",
    "        phraseTextData.append(line.strip())\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united states',\n",
       " 'directs the secretary',\n",
       " 'sets forth',\n",
       " 'internal revenue',\n",
       " 'fiscal year',\n",
       " 'authorizes the secretary',\n",
       " 'social security',\n",
       " 'authorizes appropriations',\n",
       " 'requires the secretary',\n",
       " 'expresses the sense']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnedPhrases[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['park and recreation',\n",
       " 'indian country',\n",
       " 'authorizes_the_secretary to require',\n",
       " 'honors the life',\n",
       " 'amends_federal_law relating',\n",
       " 'drug_control and system improvement',\n",
       " 'tangible things',\n",
       " 'radiation exposure',\n",
       " 'various types',\n",
       " 'economic conditions']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnedPhrases[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provides that effective from january_3',\n",
       " '1973',\n",
       " 'the joint_committee created to make the necessary arrangements for the inauguration of the president-elect_and_vice_president-elect of the united_states on the 20th day of january 1973',\n",
       " 'is hereby continued and for such purpose shall have the same power and authority as that conferred by senate concurrent_resolution 63',\n",
       " 'of the ninety-second congress',\n",
       " 'makes_it_the_sense_of_the_congress that the pollution of waters all over the world is a matter of vital concern to all_nations and should be dealt with as a matter of the highest_priority',\n",
       " 'makes_it_the_sense_of_the_congress that the president',\n",
       " 'acting through the united_states delegation to the united national_conference on the human_environment',\n",
       " 'should take such steps as may be necessary to propose an international_agreement',\n",
       " 'or amendments to existing international_agreements',\n",
       " 'as may be appropriate',\n",
       " 'providing for coordinated international activites to prohibit the disposal of munitions',\n",
       " 'chemicals',\n",
       " 'chemical_munitions',\n",
       " 'military material']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phraseTextData[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add text with learned phrases back into data frame\n",
    "textFrame['TextWithPhrases'] = phraseTextData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "      <th>TextWithPhrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>5</td>\n",
       "      <td>as may be appropriate</td>\n",
       "      <td>as may be appropriate</td>\n",
       "      <td>as may be appropriate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>6</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "      <td>providing for coordinated international activi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>7</td>\n",
       "      <td>chemicals</td>\n",
       "      <td>chemicals</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>8</td>\n",
       "      <td>chemical munitions</td>\n",
       "      <td>chemical munitions</td>\n",
       "      <td>chemical_munitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>9</td>\n",
       "      <td>military material</td>\n",
       "      <td>military material</td>\n",
       "      <td>military material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>10</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "      <td>and any pollutants in territorial waters</td>\n",
       "      <td>and any pollutants in territorial_waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>11</td>\n",
       "      <td>contiguous zones</td>\n",
       "      <td>contiguous zones</td>\n",
       "      <td>contiguous zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>12</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "      <td>the deep seabed or any international waters</td>\n",
       "      <td>the deep_seabed or any international waters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>13</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "      <td>and otherwise to prevent the pollution of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>0</td>\n",
       "      <td>Establishes a Joint Congressional Committee on...</td>\n",
       "      <td>establishes a joint congressional committee on...</td>\n",
       "      <td>establishes a joint congressional_committee on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID  DocLine                                        CleanedText  \\\n",
       "10  hconres2-93        5                              as may be appropriate   \n",
       "11  hconres2-93        6  providing for coordinated international activi...   \n",
       "12  hconres2-93        7                                          chemicals   \n",
       "13  hconres2-93        8                                 chemical munitions   \n",
       "14  hconres2-93        9                                  military material   \n",
       "15  hconres2-93       10           and any pollutants in territorial waters   \n",
       "16  hconres2-93       11                                   contiguous zones   \n",
       "17  hconres2-93       12        the deep seabed or any international waters   \n",
       "18  hconres2-93       13  and otherwise to prevent the pollution of the ...   \n",
       "19  hconres3-93        0  Establishes a Joint Congressional Committee on...   \n",
       "\n",
       "                                        LowercaseText  \\\n",
       "10                              as may be appropriate   \n",
       "11  providing for coordinated international activi...   \n",
       "12                                          chemicals   \n",
       "13                                 chemical munitions   \n",
       "14                                  military material   \n",
       "15           and any pollutants in territorial waters   \n",
       "16                                   contiguous zones   \n",
       "17        the deep seabed or any international waters   \n",
       "18  and otherwise to prevent the pollution of the ...   \n",
       "19  establishes a joint congressional committee on...   \n",
       "\n",
       "                                      TextWithPhrases  \n",
       "10                              as may be appropriate  \n",
       "11  providing for coordinated international activi...  \n",
       "12                                          chemicals  \n",
       "13                                 chemical_munitions  \n",
       "14                                  military material  \n",
       "15           and any pollutants in territorial_waters  \n",
       "16                                   contiguous zones  \n",
       "17        the deep_seabed or any international waters  \n",
       "18  and otherwise to prevent the pollution of the ...  \n",
       "19  establishes a joint congressional_committee on...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFrame[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the joint_committee created to make the necessary arrangements for the inauguration of the president-elect_and_vice_president-elect of the united_states on the 20th day of january 1973'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFrame['TextWithPhrases'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most Common Surface Form of Each Lower-Cased Word and Phrase\n",
    "\n",
    "The text data is lower cased in order to merge differently cased versions of the same word prior to doing topic modeling. In order to generate summaries of topics that will be learned, we would like to present the most likely surface form of a word to the user. For example, if a proper noun is converted to all lowercase characters for latent topic modeling, we want the user to see this proper name with its proper capitalization within summaries. The MapVocabToSurfaceForms() function achieves this by mapping every lowercased word and phrase used during latent topic modeling to its most common surface form in the text collection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "e.g., in topic (topic_modeling, text, categorize, lda, plsi), we want the lda to be presented as LDA, and the plsi to be presented as pLSI.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MapVocabToSurfaceForms(textData):\n",
    "    surfaceFormCountHash = {}\n",
    "    vocabToSurfaceFormHash = {}\n",
    "    regexUnderBar = re.compile('_')\n",
    "    regexSpace = re.compile(' +')\n",
    "    regexClean = re.compile('^ +| +$')\n",
    "    \n",
    "    # First go through every line of text, align each word/phrase with\n",
    "    # it's surface form and count the number of times each surface form occurs\n",
    "    for i in range(0,len(textData)):    \n",
    "        origWords = regexSpace.split(regexClean.sub(\"\",str(textData['CleanedText'][i])))\n",
    "        numOrigWords = len(origWords)\n",
    "        newWords = regexSpace.split(regexClean.sub(\"\",str(textData['TextWithPhrases'][i])))\n",
    "        numNewWords = len(newWords)\n",
    "        origIndex = 0\n",
    "        newIndex = 0\n",
    "        while newIndex < numNewWords:\n",
    "            # Get the next word or phrase in the lower-cased text with phrases and\n",
    "            # match it to the original form of the same n-gram in the original text\n",
    "            newWord = newWords[newIndex]\n",
    "            phraseWords = regexUnderBar.split(newWord)\n",
    "            numPhraseWords = len(phraseWords)\n",
    "            matchedWords = \" \".join(origWords[origIndex:(origIndex+numPhraseWords)])\n",
    "            origIndex += numPhraseWords\n",
    "                \n",
    "            # Now do the bookkeeping for collecting the different surface form \n",
    "            # variations present for each lowercased word or phrase\n",
    "            if newWord in vocabToSurfaceFormHash:\n",
    "                vocabToSurfaceFormHash[newWord].add(matchedWords)\n",
    "            else:\n",
    "                vocabToSurfaceFormHash[newWord] = set([matchedWords])\n",
    "\n",
    "            # Increment the counter for this surface form\n",
    "            if matchedWords not in surfaceFormCountHash:\n",
    "                surfaceFormCountHash[matchedWords] = 1\n",
    "            else:\n",
    "                surfaceFormCountHash[matchedWords] += 1\n",
    "   \n",
    "            if ( len(newWord) != len(matchedWords)):\n",
    "                print (\"##### Error #####\")\n",
    "                print (\"Bad Match: %s ==> %s \" % (newWord,matchedWords))\n",
    "                print (\"From line: %s\" % textData['TextWithPhrases'][i])\n",
    "                print (\"Orig text: %s\" % textData['CleanedText'][i])\n",
    "                \n",
    "                return False\n",
    "\n",
    "            newIndex += 1\n",
    "    # After aligning and counting, select the most common surface form for each\n",
    "\n",
    "    # word/phrase to be the canonical example shown to the user for that word/phrase\n",
    "    for ngram in vocabToSurfaceFormHash.keys():\n",
    "        maxCount = 0\n",
    "        bestSurfaceForm = \"\"\n",
    "        for surfaceForm in vocabToSurfaceFormHash[ngram]:\n",
    "            if surfaceFormCountHash[surfaceForm] > maxCount:\n",
    "                maxCount = surfaceFormCountHash[surfaceForm]\n",
    "                bestSurfaceForm = surfaceForm\n",
    "        if ngram != \"\":\n",
    "            if bestSurfaceForm == \"\":\n",
    "                print (\"Warning: NULL surface form for ngram '%s'\" % ngram)\n",
    "            else:\n",
    "                vocabToSurfaceFormHash[ngram] = bestSurfaceForm\n",
    "    \n",
    "    return vocabToSurfaceFormHash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if True:\n",
    "    vocabToSurfaceFormHash = MapVocabToSurfaceForms(textFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the mapping between model vocabulary and surface form mapping\n",
    "tsvFile = os.path.join(\"./Data\", \"Vocab2SurfaceFormMapping.tsv\")\n",
    "\n",
    "saveSurfaceFormFile = True\n",
    "\n",
    "if saveSurfaceFormFile:\n",
    "    with open(tsvFile, 'w', encoding='utf-8') as fp:\n",
    "        for key, val in vocabToSurfaceFormHash.items():\n",
    "            if key != \"\":\n",
    "                strOut = \"%s\\t%s\\n\" % (key, val)\n",
    "                fp.write(strOut)\n",
    "else:\n",
    "    # Load surface form mappings here\n",
    "    vocabToSurfaceFormHash = {}\n",
    "    fp = open(tsvFile, encoding='utf-8')\n",
    "\n",
    "    # Each line in the file has two tab separated fields;\n",
    "    # the first is the vocabulary item used during modeling\n",
    "    # and the second is its most common surface form in the \n",
    "    # original data\n",
    "    for stringIn in fp.readlines():\n",
    "        fields = stringIn.strip().split(\"\\t\")\n",
    "        if len(fields) != 2:\n",
    "            print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
    "        elif fields[0] == \"\" or fields[1] == \"\":\n",
    "            print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
    "        else:\n",
    "            vocabToSurfaceFormHash[fields[0]] = fields[1]\n",
    "    fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security\n",
      "Declares\n",
      "mental health\n",
      "El Salvador\n",
      "Department of the Interior\n"
     ]
    }
   ],
   "source": [
    "print(vocabToSurfaceFormHash['security'])\n",
    "print(vocabToSurfaceFormHash['declares'])\n",
    "print(vocabToSurfaceFormHash['mental_health'])\n",
    "print(vocabToSurfaceFormHash['el_salvador'])\n",
    "print(vocabToSurfaceFormHash['department_of_the_interior'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the Full Processed Text of Each Document and Put it into a New Frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "apply all preprocessing including the phrase stuff we have done in this part \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def ReconstituteDocsFromChunks(textData, idColumnName, textColumnName):\n",
    "    dataOut = []\n",
    "    \n",
    "    currentDoc = \"\"\n",
    "    currentDocID = \"\"\n",
    "    \n",
    "    for i in range(0,len(textData)):\n",
    "        textChunk = textData[textColumnName][i]\n",
    "        docID = str(textData[idColumnName][i])\n",
    "        if docID != currentDocID:\n",
    "            if currentDocID != \"\":\n",
    "                dataOut.append([currentDocID, currentDoc])\n",
    "            currentDoc = textChunk\n",
    "            currentDocID = docID\n",
    "        else:\n",
    "            currentDoc += \" \" + textChunk\n",
    "    dataOut.append([currentDocID,currentDoc])\n",
    "    \n",
    "    frameOut = pandas.DataFrame(dataOut, columns=['DocID','ProcessedText'])\n",
    "    \n",
    "    return frameOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if True:\n",
    "    docsFrame = ReconstituteDocsFromChunks(textFrame, 'DocID', 'TextWithPhrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveProcessedText = True\n",
    "\n",
    "# Save processed text for each document back out to a TSV file\n",
    "if saveProcessedText:\n",
    "    docsFrame.to_csv(os.path.join(\"./Data\", 'CongressionalDocsProcessed.tsv'),  \n",
    "                        sep='\\t', index=False)\n",
    "else: \n",
    "    docsFrame = pandas.read_csv(\"./Data\", 'CongressionalDocsProcessed.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-93</td>\n",
       "      <td>provides that effective from january_3 1973 th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres2-93</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres3-93</td>\n",
       "      <td>establishes a joint congressional_committee on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres4-93</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres5-93</td>\n",
       "      <td>makes_it_the_sense_of_the_congress that the co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DocID                                      ProcessedText\n",
       "0  hconres1-93  provides that effective from january_3 1973 th...\n",
       "1  hconres2-93  makes_it_the_sense_of_the_congress that the po...\n",
       "2  hconres3-93  establishes a joint congressional_committee on...\n",
       "3  hconres4-93  makes_it_the_sense_of_the_congress that the pr...\n",
       "4  hconres5-93  makes_it_the_sense_of_the_congress that the co..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsFrame[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'makes_it_the_sense_of_the_congress that the pollution of waters all over the world is a matter of vital concern to all_nations and should be dealt with as a matter of the highest_priority makes_it_the_sense_of_the_congress that the president acting through the united_states delegation to the united national_conference on the human_environment should take such steps as may be necessary to propose an international_agreement or amendments to existing international_agreements as may be appropriate providing for coordinated international activites to prohibit the disposal of munitions chemicals chemical_munitions military material and any pollutants in territorial_waters contiguous zones the deep_seabed or any international waters and otherwise to prevent the pollution of the waters of the world'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsFrame['ProcessedText'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Apply Rules to New Documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#053582'>\n",
    "<br>\n",
    "A method that transforms previously unseen text to fit our learned phrased  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ApplyPhraseRewritesInPlace(textFrame, textColumnName, phraseRules):\n",
    "    \n",
    "    # Make sure we have phrase to add\n",
    "    numPhraseRules = len(phraseRules)\n",
    "    if numPhraseRules == 0: \n",
    "        print (\"Warning: phrase rule lise is empty - no phrases being applied to text data\")\n",
    "        return\n",
    "    \n",
    "    # Get text data column from frame\n",
    "    textData = textFrame[textColumnName]\n",
    "    numLines = len(textData)\n",
    "    \n",
    "    # Add leading and trailing spaces to make regex matching easier\n",
    "    for i in range(0,numLines):\n",
    "        textData[i] = \" \" + textData[i] + \" \"  \n",
    "\n",
    "    # Precompile the regex for finding spaces in ngram phrases\n",
    "    regexSpace = re.compile(' ')\n",
    "   \n",
    "    # Initialize some bookkeeping variables\n",
    "\n",
    "    # Iterate through full set of phrases to find sets of \n",
    "    # non-conflicting phrases that can be apply simultaneously\n",
    "    index = 0\n",
    "    outerStop = False\n",
    "    while not outerStop:\n",
    "       \n",
    "        # Create empty hash tables to keep track of phrase overlap conflicts\n",
    "        leftConflictHash = {}\n",
    "        rightConflictHash = {}\n",
    "        prevConflictHash = {}\n",
    "    \n",
    "        # Create an empty hash table collecting the next set of rewrite rules\n",
    "        # to be applied during this iteration of phrase rewriting\n",
    "        phraseRewriteHash = {}\n",
    "    \n",
    "        # Progress through phrases until the next conflicting phrase is found\n",
    "        innerStop = 0\n",
    "        numPhrasesAdded = 0\n",
    "        while not innerStop:\n",
    "        \n",
    "            # Get the next phrase to consider adding to the phrase list\n",
    "            nextPhrase = phraseRules[index]            \n",
    "            \n",
    "            # Extract the left and right sides of the phrase to use\n",
    "            # in checks for phrase overlap conflicts\n",
    "            ngramArray = nextPhrase.split()\n",
    "            leftWord = ngramArray[0]\n",
    "            rightWord = ngramArray[-1] \n",
    "\n",
    "            # Stop if we reach any phrases that conflicts with earlier phrases in this iteration\n",
    "            # These ngram phrases will be reconsidered in the next iteration\n",
    "            if ((leftWord in leftConflictHash) or (rightWord in rightConflictHash) \n",
    "                or (leftWord in prevConflictHash) or (rightWord in prevConflictHash)): \n",
    "                innerStop = True\n",
    "                \n",
    "            # If no conflict exists then add this phrase into the list of phrase rewrites     \n",
    "            else: \n",
    "                # Create the output compound word version of the phrase\n",
    "                                \n",
    "                outputPhrase = regexSpace.sub(\"_\",nextPhrase);\n",
    "                \n",
    "                # Keep track of all context words that might conflict with upcoming\n",
    "                # propose phrases (even when phrases are skipped instead of added)\n",
    "                leftConflictHash[rightWord] = 1\n",
    "                rightConflictHash[leftWord] = 1\n",
    "                prevConflictHash[outputPhrase] = 1           \n",
    "                \n",
    "                # Add extra space to input an output versions of the current phrase \n",
    "                # to make the regex rewrite easier\n",
    "                outputPhrase = \" \" + outputPhrase\n",
    "                lastAddedPhrase = \" \" + nextPhrase\n",
    "                \n",
    "                # Add the phrase to the rewrite hash\n",
    "                phraseRewriteHash[lastAddedPhrase] = outputPhrase\n",
    "                  \n",
    "                # Increment to next phrase\n",
    "                index += 1\n",
    "                numPhrasesAdded  += 1\n",
    "    \n",
    "                # Stop if we've reached the end of the phrases list\n",
    "                if index >= numPhraseRules:\n",
    "                    innerStop = True\n",
    "                    outerStop = True\n",
    "                    \n",
    "        # Now do the phrase rewrites over the entire set of text data\n",
    "        if numPhrasesAdded == 1:\n",
    "        \n",
    "            # If only one phrase to add use a single regex rule to do this phrase rewrite        \n",
    "            outputPhrase = phraseRewriteHash[lastAddedPhrase]\n",
    "            regexPhrase = re.compile (r'%s(?= )' % re.escape(lastAddedPhrase)) \n",
    "        \n",
    "            # Apply the regex over the full data set\n",
    "            for j in range(0,numLines):\n",
    "                textData[j] = regexPhrase.sub(outputPhrase, textData[j])\n",
    "        \n",
    "        elif numPhrasesAdded > 1:\n",
    "            # Compile a single regex rule from the collected set of phrase rewrites for this iteration\n",
    "            regexPhrase = re.compile(r'%s(?= )' % \"|\".join(map(re.escape, phraseRewriteHash.keys())))\n",
    "            \n",
    "            # Apply the regex over the full data set\n",
    "            for i in range(0,numLines):\n",
    "                # The regex substituion looks up the output string rewrite  \n",
    "                # in the hash table for each matched input phrase regex\n",
    "                textData[i] = regexPhrase.sub(lambda mo: phraseRewriteHash[mo.string[mo.start():mo.end()]], textData[i]) \n",
    "    \n",
    "    # Remove the space padding at the start and end of each line\n",
    "    regexSpacePadding = re.compile('^ +| +$')\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i] = regexSpacePadding.sub(\"\",textData[i])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the president_of_the_united_states appoints the secretary_of_labor to lead the department_of_labor\n",
      "the speaker_of_the_house_of_representatives is elected each session by the members_of_the_house\n",
      "the president_pro_tempore of the the u.s. senate resides over the senate when the vice_president is absent\n"
     ]
    }
   ],
   "source": [
    "testText = [\"the president of the united states appoints the secretary of labor to lead the department of labor\", \n",
    "            \"the speaker of the house of representatives is elected each session by the members of the house\",\n",
    "            \"the president pro tempore of the the u.s. senate resides over the senate when the vice president is absent\"]\n",
    "\n",
    "testFrame = pandas.DataFrame(testText, columns=['TestText'])      \n",
    "\n",
    "ApplyPhraseRewritesInPlace(testFrame, 'TestText', learnedPhrases)\n",
    "\n",
    "print(testFrame['TestText'][0])\n",
    "print(testFrame['TestText'][1])\n",
    "print(testFrame['TestText'][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next\n",
    "\n",
    "The phrase learning step is finished. The next step will be topic modeling which will be in the third notebook of the series: [`3_Topic_Model_Training.ipynb`](./3_Topic_Model_Training.ipynb)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
